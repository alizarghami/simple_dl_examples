{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MNIST('./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = MNIST('./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=20, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyAutoEncoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.up = nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(20, 10, 5, padding=4)\n",
    "        self.conv4 = nn.Conv2d(10, 1, 5, padding=4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        def encode(x):\n",
    "            x = f.relu(self.conv1(x))\n",
    "            x = self.pool(x)\n",
    "            x = f.relu(self.conv2(x))\n",
    "            x = self.pool(x)\n",
    "            return x\n",
    "        \n",
    "        def decode(x):\n",
    "            x = self.up(x)\n",
    "            x = f.relu(self.conv3(x))\n",
    "            x = self.up(x)\n",
    "            x = f.relu(self.conv4(x))\n",
    "            return x\n",
    "        \n",
    "        x = encode(x)\n",
    "        x = decode(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyAutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a data before training\n",
    "sample, label = next(iter(test_loader))\n",
    "image = np.squeeze(sample.numpy())\n",
    "\n",
    "output = model.forward(sample)\n",
    "res = np.squeeze(output.data.numpy())\n",
    "\n",
    "fig1 = plt.figure('Before training')\n",
    "ax1 = fig1.add_subplot(1,2,1)\n",
    "ax1.imshow(image)\n",
    "ax2 = fig1.add_subplot(1,2,2)\n",
    "ax2.imshow(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training the network\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_epoch = 20\n",
    "running_loss = 0\n",
    "loss_over_time =[]\n",
    "for epoch in range(n_epoch):\n",
    "    for batch_i, (X_train, y_train) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(X_train)\n",
    "        \n",
    "        loss = loss_function(output, X_train)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if batch_i % 1000 == 999:\n",
    "            avg_loss = running_loss/1000\n",
    "            loss_over_time.append(avg_loss)\n",
    "            # record and print the avg loss over the 1000 batches\n",
    "            print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, avg_loss))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure('Loss over time')\n",
    "plt.plot(loss_over_time)\n",
    "plt.xlabel('1000\\'s of batches')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim(0, 0.05) # consistent scale\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on a data after training\n",
    "output = model.forward(sample)\n",
    "res = np.squeeze(output.data.numpy())\n",
    "\n",
    "fig3 = plt.figure('After training')\n",
    "ax1 = fig3.add_subplot(1,2,1)\n",
    "ax1.imshow(image)\n",
    "ax2 = fig3.add_subplot(1,2,2)\n",
    "ax2.imshow(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "version = 2\n",
    "model_dir = 'saved_models/'\n",
    "model_name = 'mnist_conv_autoencoder_v{}.pt'.format(version)\n",
    "\n",
    "# after training, save your model parameters in the dir 'saved_models'\n",
    "# when you're ready, un-comment the line below\n",
    "torch.save(model.state_dict(), model_dir+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
